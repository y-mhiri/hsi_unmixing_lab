{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Exercise 3: Constrained Least Squares Unmixing\n",
    "\n",
    "## Objectives\n",
    "- Implement sum-to-one constrained least squares using Lagrange multipliers\n",
    "- Understand and implement projected gradient descent\n",
    "- Apply simplex projection for full constraints\n",
    "- Compare different constraint strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_loader import HyperspectralDataLoader\n",
    "from visualization import HSIVisualizer\n",
    "from optimization import HyperspectralUnmixer\n",
    "from metrics import UnmixingEvaluator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load Data\n",
    "\n",
    "Load the data and previous results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = HyperspectralDataLoader(\"../data\")\n",
    "hsi_data, ground_truth = loader.load_pavia()\n",
    "Y, _ = loader.vectorize_data()\n",
    "S = np.load('../data/extracted_endmembers.npy')\n",
    "endmember_names = np.load('../data/endmember_names.npy', allow_pickle=True)\n",
    "\n",
    "# Load unconstrained results\n",
    "A_unconstrained = np.load('../data/unconstrained_abundances.npy')\n",
    "\n",
    "print(f\"Data loaded: Y{Y.shape}, S{S.shape}\")\n",
    "print(f\"Unconstrained abundances: {A_unconstrained.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Sum-to-One Constrained Least Squares\n",
    "\n",
    "**Mathematical Background:**\n",
    "\n",
    "The sum-to-one constrained problem is:\n",
    "$$\\min_{\\mathbf{A}} \\|\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\|_F^2 \\quad \\text{subject to} \\quad \\mathbf{1}^T\\mathbf{A} = \\mathbf{1}^T$$\n",
    "\n",
    "Using Lagrange multipliers, the solution for pixel $p$ is:\n",
    "$$\\mathbf{a}_p^* = (\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{S}^T\\mathbf{y}_p + (\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{1}\\lambda_p^*$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_to_one_constrained_manual(S, Y):\n",
    "    \"\"\"\n",
    "    Manual implementation of sum-to-one constrained least squares.\n",
    "    \n",
    "    TODO: Implement using Lagrange multipliers\n",
    "    \"\"\"\n",
    "    num_endmembers, num_pixels = S.shape[1], Y.shape[1]\n",
    "    \n",
    "    # Precompute matrices\n",
    "    StS = S.T@S # YOUR CODE HERE\n",
    "    StY = S.T@Y# YOUR CODE HERE\n",
    "    ones = np.ones((num_endmembers,1)) # YOUR CODE HERE (vector of ones)\n",
    "    \n",
    "    try:\n",
    "        StS_inv = np.linalg.inv(StS) # YOUR CODE HERE\n",
    "    except np.linalg.LinAlgError:\n",
    "        StS_inv = np.linalg.pinv(StS)\n",
    "    \n",
    "    # Unconstrained solution\n",
    "    A_unconstrained = StS_inv @ StY # YOUR CODE HERE\n",
    "    \n",
    "    # Compute lambda for each pixel\n",
    "    # lambda = (1 - 1^T A_unconstrained) / (1^T (S^T S)^(-1) 1)\n",
    "    denominator = ones.T @ StS_inv @ ones # YOUR CODE HERE\n",
    "    numerator = 1 - ones.T @ A_unconstrained # YOUR CODE HERE\n",
    "    \n",
    "    lambda_values = numerator / denominator\n",
    "    \n",
    "    # Final constrained solution\n",
    "    A = A_unconstrained + StS_inv @ ones @ lambda_values  # YOUR CODE HERE\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Test implementation\n",
    "A_sum_manual = sum_to_one_constrained_manual(S, Y)\n",
    "\n",
    "# Compare with library\n",
    "unmixer = HyperspectralUnmixer()\n",
    "A_sum_library = unmixer.sum_to_one_constrained_ls(S, Y)\n",
    "\n",
    "difference = np.max(np.abs(A_sum_manual - A_sum_library))\n",
    "print(f\"Implementation difference: {difference:.2e}\")\n",
    "\n",
    "# Check sum-to-one constraint\n",
    "sums = np.sum(A_sum_library, axis=0)\n",
    "print(f\"Sum constraint violation: {np.max(np.abs(sums - 1)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Projected Gradient Descent Implementation\n",
    "\n",
    "Implement projected gradient descent for different constraint types.\n",
    "\n",
    "First, we will implement a Non negative Constrained Least Square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_descent_manual(S, Y, max_iter=500, step_size=0.001, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Manual implementation of projected gradient descent.\n",
    "    \n",
    "    TODO: Implement the algorithm\n",
    "    \"\"\"\n",
    "    num_endmembers, num_pixels = S.shape[1], Y.shape[1]\n",
    "\n",
    "    ## Initialization \n",
    "    A = unmixer.unconstrained_least_squares(S, Y)\n",
    "    A = np.maximum(A, 0) # YOUR CODE HERE\n",
    "\n",
    "    # Precompute for efficiency\n",
    "    StS = S.T@S# YOUR CODE HERE\n",
    "    StY = S.T@Y# YOUR CODE HERE\n",
    "    \n",
    "    objective_values = []\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Compute objective\n",
    "        residual = S @ A - Y# YOUR CODE HERE\n",
    "        objective = 0.5 * np.sum(residual**2) # YOUR CODE HERE\n",
    "        objective_values.append(objective)\n",
    "        \n",
    "        # Compute gradient: grad_A = S^T (SA - Y)\n",
    "        gradient = StS @ A - StY\n",
    "        \n",
    "        # Gradient descent step\n",
    "        A_new = A - step_size * gradient\n",
    "            \n",
    "        # Apply projection based on constraint type\n",
    "        A_new = np.maximum(A_new, 0) # YOUR CODE HERE\n",
    "        \n",
    "        # Check convergence\n",
    "        if iteration > 0:\n",
    "            rel_change = abs(objective_values[-1] - objective_values[-2]) / objective_values[-2]\n",
    "            if rel_change < tolerance:\n",
    "                print(f\"Converged after {iteration+1} iterations\")\n",
    "                break\n",
    "        \n",
    "        A = A_new\n",
    "    \n",
    "    return A, objective_values\n",
    "\n",
    "# Test implementation with non-negativity constraints\n",
    "print(\"Testing non-negativity constrained unmixing...\")\n",
    "\n",
    "StS = S.T @ S\n",
    "max_eig = np.max(np.linalg.eigvals(StS))\n",
    "A_nn_manual, obj_nn = projected_gradient_descent_manual(S, Y, max_iter=100, step_size=.99/max_eig)\n",
    "\n",
    "print(f\"Final objective: {obj_nn[-1]:.6f}\")\n",
    "print(f\"Minimum abundance: {np.min(A_nn_manual):.6f}\")\n",
    "print(f\"Convergence in {len(obj_nn)} iterations\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(obj_nn)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"objective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the reconstruction using the function used in last Exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Plot abundance maps\n",
    "visualizer = HSIVisualizer()\n",
    "evaluator = UnmixingEvaluator()\n",
    "height, width = hsi_data.shape[:2]\n",
    "\n",
    "# YOUR CODE HERE to plot abundance maps\n",
    "visualizer.plot_abundance_maps(A_nn_manual, ground_truth, [height, width], endmember_names, cmap='viridis')\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Evaluate reconstruction\n",
    "rgb_bands = loader.get_rgb_bands()\n",
    "results = evaluator.evaluate_reconstruction(S,A_nn_manual, Y, [height, width])\n",
    "\n",
    "print(\"Reconstruction Quality Metrics:\")\n",
    "print(f\"  Mean SAM (degrees): {results['mean_sam_degrees']:.4f}\")\n",
    "print(f\"  RMSE: {results['rmse']:.6f}\")\n",
    "if 'ssim_mean' in results:\n",
    "    print(f\"  Mean SSIM: {results['ssim_mean']:.4f}\")\n",
    "print(f\"  SNR (dB): {results['snr_db']:.2f}\")\n",
    "\n",
    "# TODO: Reconstruct hyperspectral data\n",
    "Y_reconstructed = S@A_nn_manual\n",
    "hsi_reconstructed = Y_reconstructed.T.reshape(height, width, -1)\n",
    "\n",
    "# TODO: Plot comparison\n",
    "# YOUR CODE HERE\n",
    "visualizer.plot_reconstruction_comparison(hsi_data, hsi_reconstructed, rgb_bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement Simplex Projection\n",
    "\n",
    "The simplex projection is crucial for handling both non-negativity and sum-to-one constraints simultaneously.\n",
    "We will first implement a simplex projection algorithm and then modify the projected gradient descent function to project the right constraint set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the simplex projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_simplex_manual(v):\n",
    "    \"\"\"\n",
    "    Project vector v onto the unit simplex using Duchi et al. algorithm.\n",
    "    \n",
    "    TODO: Implement the simplex projection algorithm\n",
    "    \"\"\"\n",
    "    n = len(v)\n",
    "    \n",
    "    # Step 1: Sort in descending order\n",
    "    u = np.sort(v)[::-1] # YOUR CODE HERE\n",
    "    \n",
    "    # Step 2: Find rho\n",
    "    cssv = np.cumsum(u) - 1.0 # YOUR CODE HERE (cumulative sum - 1)\n",
    "    ind = np.arange(n) + 1 # YOUR CODE HERE (indices 1, 2, ..., n)\n",
    "    cond = u - cssv / ind > 0 # YOUR CODE HERE (condition u - cssv/ind > 0)\n",
    "    \n",
    "    if np.any(cond):\n",
    "        rho = np.where(cond)[0][-1] # YOUR CODE HERE\n",
    "        theta = cssv[rho] / (rho + 1.0) # YOUR CODE HERE\n",
    "    else:\n",
    "        theta = 0.0\n",
    "    \n",
    "    # Step 3: Project\n",
    "    w = np.maximum(v - theta, 0) # YOUR CODE HERE\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Test simplex projection\n",
    "test_vector = np.array([0.5, -0.2, 0.8, 0.1])\n",
    "projected = project_onto_simplex_manual(test_vector)\n",
    "\n",
    "print(f\"Original: {test_vector}\")\n",
    "print(f\"Projected: {projected}\")\n",
    "print(f\"Sum: {np.sum(projected):.6f}\")\n",
    "print(f\"All non-negative: {np.all(projected >= 0)}\")\n",
    "\n",
    "# Compare with library implementation\n",
    "projected_lib = unmixer.project_onto_simplex(test_vector)\n",
    "print(f\"Difference from library: {np.max(np.abs(projected - projected_lib)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the projected gradient descent algorithm to include multiple constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_descent_manual(S, Y, constraint_type='simplex', \n",
    "                                    max_iter=500, step_size=0.001, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Manual implementation of projected gradient descent.\n",
    "    \n",
    "    TODO: Implement the algorithm\n",
    "    \"\"\"\n",
    "    num_endmembers, num_pixels = S.shape[1], Y.shape[1]\n",
    "    A = unmixer.unconstrained_least_squares(S, Y)\n",
    "\n",
    "    # Initialize abundances based on constraint type\n",
    "    if constraint_type == 'simplex':\n",
    "        # Initialize on simplex\n",
    "        for p in range(num_pixels):\n",
    "                A[:, p] = project_onto_simplex_manual(A[:, p])# YOUR CODE HERE (You can project the unconstrained solution)\n",
    "    elif constraint_type == 'non_negative':\n",
    "        A = np.maximum(A, 0) # YOUR CODE HERE\n",
    "    elif constraint_type == 'sum_to_one':\n",
    "        A = A/np.sum(A, axis=0, keepdims=True)\n",
    "    \n",
    "    # Precompute for efficiency\n",
    "    StS = S.T@S# YOUR CODE HERE\n",
    "    StY = S.T@Y# YOUR CODE HERE\n",
    "    \n",
    "    objective_values = []\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Compute objective\n",
    "        residual = S @ A - Y# YOUR CODE HERE\n",
    "        objective = 0.5 * np.sum(residual**2) # YOUR CODE HERE\n",
    "        objective_values.append(objective)\n",
    "        \n",
    "        # Compute gradient: grad_A = S^T (SA - Y)\n",
    "        gradient = StS @ A - StY\n",
    "        \n",
    "        # Gradient descent step\n",
    "        A_new = A - step_size * gradient\n",
    "            \n",
    "        # Apply projection based on constraint type\n",
    "        if constraint_type == 'non_negative':\n",
    "            A_new = np.maximum(A_new, 0) # YOUR CODE HERE\n",
    "        elif constraint_type == 'sum_to_one':\n",
    "            A_new =  A_new / np.sum(A_new, axis=0, keepdims=True) # YOUR CODE HERE\n",
    "        elif constraint_type == 'simplex':\n",
    "            for p in range(num_pixels):\n",
    "                A_new[:, p] = project_onto_simplex_manual(A_new[:, p]) # YOUR CODE HERE\n",
    "        \n",
    "        # Check convergence\n",
    "        if iteration > 0:\n",
    "            rel_change = abs(objective_values[-1] - objective_values[-2]) / objective_values[-2]\n",
    "            if rel_change < tolerance:\n",
    "                print(f\"Converged after {iteration+1} iterations\")\n",
    "                break\n",
    "        \n",
    "        A = A_new\n",
    "    \n",
    "    return A, objective_values\n",
    "\n",
    "# Test implementation with non-negativity constraints\n",
    "print(\"Testing non-negativity constrained unmixing...\")\n",
    "\n",
    "StS = S.T @ S\n",
    "max_eig = np.max(np.linalg.eigvals(StS))\n",
    "A_nn_manual, obj_nn = projected_gradient_descent_manual(S, Y, 'non_negative', max_iter=100, step_size=.99/max_eig)\n",
    "\n",
    "print(f\"Final objective: {obj_nn[-1]:.6f}\")\n",
    "print(f\"Minimum abundance: {np.min(A_nn_manual):.6f}\")\n",
    "print(f\"Convergence in {len(obj_nn)} iterations\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(obj_nn)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"objective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Compare All Constraint Methods\n",
    "\n",
    "Apply and compare all constraint strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all methods\n",
    "print(\"Applying all constraint methods...\")\n",
    "\n",
    "methods = {\n",
    "    'Unconstrained': A_unconstrained,\n",
    "    'Sum-to-one': A_sum_library,\n",
    "}\n",
    "\n",
    "# Non-negativity constrained\n",
    "A_nn, obj_nn = unmixer.projected_gradient_descent(S, Y, 'non_negative', max_iter=200)\n",
    "methods['Non-negative'] = A_nn\n",
    "\n",
    "# Fully constrained (simplex)\n",
    "A_simplex, obj_simplex = unmixer.fully_constrained_least_squares(S, Y, max_iter=200)\n",
    "methods['Simplex'] = A_simplex\n",
    "\n",
    "# Evaluate all methods\n",
    "evaluator = UnmixingEvaluator()\n",
    "height, width = hsi_data.shape[:2]\n",
    "rgb_bands = loader.get_rgb_bands()\n",
    "\n",
    "comparison_results = {}\n",
    "abundance_results = {}\n",
    "\n",
    "for name, A in methods.items():\n",
    "    # Reconstruction metrics\n",
    "    results = evaluator.evaluate_reconstruction(S, A, Y, (height, width), rgb_bands)\n",
    "    comparison_results[name] = results\n",
    "    \n",
    "    # Abundance constraint metrics\n",
    "    abundance_stats = evaluator.evaluate_abundances(A)\n",
    "    abundance_results[name] = abundance_stats\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"  SAM (degrees): {results['mean_sam_degrees']:.4f}\")\n",
    "    print(f\"  RMSE: {results['rmse']:.6f}\")\n",
    "    print(f\"  Fraction negative: {abundance_stats['fraction_negative']:.4f}\")\n",
    "    print(f\"  Mean sum deviation: {abundance_stats['mean_sum_deviation']:.4f}\")\n",
    "\n",
    "# Compare methods table\n",
    "evaluator.compare_methods(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Visualize Convergence\n",
    "\n",
    "Plot the convergence behavior of iterative methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot convergence curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Non-negativity convergence\n",
    "axes[0].semilogy(obj_nn, 'b-', linewidth=2, label='Non-negative')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Objective Value')\n",
    "axes[0].set_title('Non-negativity Constrained Convergence')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Simplex convergence\n",
    "axes[1].semilogy(obj_simplex, 'r-', linewidth=2, label='Simplex')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Objective Value')\n",
    "axes[1].set_title('Simplex Constrained Convergence')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convergence analysis\n",
    "print(f\"Non-negative convergence rate: {(obj_nn[0] - obj_nn[-1])/obj_nn[0]:.4f}\")\n",
    "print(f\"Simplex convergence rate: {(obj_simplex[0] - obj_simplex[-1])/obj_simplex[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Visualize Abundance Maps\n",
    "\n",
    "Compare abundance maps from different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot abundance maps for comparison\n",
    "visualizer = HSIVisualizer()\n",
    "\n",
    "# Plot simplex-constrained abundances (best method)\n",
    "print(\"Simplex-Constrained Abundance Maps:\")\n",
    "visualizer.plot_abundance_maps(A_simplex, (height, width), endmember_names)\n",
    "\n",
    "# Compare abundance statistics\n",
    "print(\"\\nAbundance Statistics Comparison:\")\n",
    "visualizer.plot_abundance_statistics(A_simplex, endmember_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Analyze Constraint Satisfaction\n",
    "\n",
    "Quantitatively analyze how well each method satisfies physical constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create constraint satisfaction analysis\n",
    "constraint_analysis = {}\n",
    "\n",
    "for name, A in methods.items():\n",
    "    # Non-negativity\n",
    "    negative_count = np.sum(A < 0)\n",
    "    total_values = A.size\n",
    "    negative_fraction = negative_count / total_values\n",
    "    \n",
    "    # Sum-to-one\n",
    "    sums = np.sum(A, axis=0)\n",
    "    sum_violations = np.sum(np.abs(sums - 1) > 0.01)  # 1% tolerance\n",
    "    sum_violation_fraction = sum_violations / len(sums)\n",
    "    \n",
    "    # Simplex constraint (both together)\n",
    "    in_simplex = (A >= 0).all(axis=0) & (np.abs(sums - 1) < 0.01)\n",
    "    simplex_fraction = np.sum(in_simplex) / len(sums)\n",
    "    \n",
    "    constraint_analysis[name] = {\n",
    "        'negative_fraction': negative_fraction,\n",
    "        'sum_violation_fraction': sum_violation_fraction,\n",
    "        'simplex_fraction': simplex_fraction\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"Constraint Satisfaction Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Method':<15} {'Negative %':<12} {'Sum Viol %':<12} {'In Simplex %':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, stats in constraint_analysis.items():\n",
    "    print(f\"{name:<15} {stats['negative_fraction']*100:<11.2f} \"\n",
    "          f\"{stats['sum_violation_fraction']*100:<11.2f} \"\n",
    "          f\"{stats['simplex_fraction']*100:<11.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Step Size Analysis\n",
    "\n",
    "Investigate the effect of step size on convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different step sizes\n",
    "step_sizes = [0.0001, 0.001, 0.01, 0.1]\n",
    "step_results = {}\n",
    "\n",
    "# Use a subset of data for faster testing\n",
    "Y_subset = Y[:, :1000]  # First 1000 pixels\n",
    "\n",
    "for step in step_sizes:\n",
    "    print(f\"Testing step size {step}...\")\n",
    "    \n",
    "    try:\n",
    "        A_test, obj_test = unmixer.projected_gradient_descent(\n",
    "            S, Y_subset, 'simplex', max_iter=100, step_size=step, tolerance=1e-6)\n",
    "        \n",
    "        step_results[step] = {\n",
    "            'final_objective': obj_test[-1],\n",
    "            'iterations': len(obj_test),\n",
    "            'convergence_rate': (obj_test[0] - obj_test[-1]) / obj_test[0]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed with step size {step}: {e}\")\n",
    "        step_results[step] = {'failed': True}\n",
    "\n",
    "# Display results\n",
    "print(\"\\nStep Size Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for step, result in step_results.items():\n",
    "    if 'failed' not in result:\n",
    "        print(f\"Step {step}: Final Obj={result['final_objective']:.2e}, \"\n",
    "              f\"Iters={result['iterations']}, Rate={result['convergence_rate']:.4f}\")\n",
    "    else:\n",
    "        print(f\"Step {step}: Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best results for next notebook\n",
    "np.save('../data/simplex_abundances.npy', A_simplex)\n",
    "np.save('../data/constraint_comparison.npy', comparison_results)\n",
    "print(\"Constrained unmixing results saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
